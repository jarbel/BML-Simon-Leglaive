{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational autoencoder for MNIST image generation\n",
    "\n",
    "Acknowledgment: Some pieces of code are taken from [this notebook](https://github.com/mlelarge/dataflowr/blob/master/CEA_EDF_INRIA/AE_filled_colab.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tested with pytorch 1.2.0 \n",
    "\n",
    "# import matplotlib\n",
    "# matplotlib.use('Qt4Agg') # if problem with PyQt5\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "my_seed = 0\n",
    "import numpy as np\n",
    "np.random.seed(my_seed)\n",
    "import torch\n",
    "torch.manual_seed(my_seed)\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from utils import to_img, plot_reconstructions_VAE, display_digits\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using gpu: True \n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print('Using gpu: %s ' % torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[```torch.utils.data.DataLoader```](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) is an iterator (an object that contains a countable number of values and that can be iterated upon) which provides the following additional features:\n",
    "- Batching the data (i.e. organizing the training samples in batches);\n",
    "- Shuffling the data;\n",
    "- Load the data in parallel using multiprocessing workers.\n",
    "\n",
    "In addition to the batch size and a boolean indicating if data should be shuffled, it takes as input an instance of [```torch.utils.data.Dataset```](https://pytorch.org/docs/stable/data.html?highlight=dataset#torch.utils.data.Dataset), which is an abstract class representing a dataset. This class has the two following methods:\n",
    "- ```__len__``` which returns the number of samples in the dataset;\n",
    "- ```__getitem__``` which returns a sample from the dataset given an index.\n",
    "\n",
    "[```torchvision.datasets```](https://pytorch.org/docs/stable/torchvision/datasets.html) contains various datasets for computer vision which are subclasses of ```torch.utils.data.Dataset```, i.e. they have ```__len__``` and ```__getitem__``` methods implemented. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('data'):\n",
    "    os.makedirs('data')\n",
    "\n",
    "train_dataset = datasets.MNIST('data', train=True, download=True, transform=transforms.ToTensor())\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=False, num_workers=1)\n",
    "\n",
    "test_dataset = datasets.MNIST('data', train=False, download=True, transform=transforms.ToTensor())\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=10, shuffle=False, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset MNIST\n",
      "    Number of datapoints: 60000\n",
      "    Root location: data\n",
      "    Split: Train\n",
      "    StandardTransform\n",
      "Transform: ToTensor()\n",
      "\n",
      "\n",
      "Dataset MNIST\n",
      "    Number of datapoints: 10000\n",
      "    Root location: data\n",
      "    Split: Test\n",
      "    StandardTransform\n",
      "Transform: ToTensor()\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show the properties of the datasets\n",
    "\n",
    "print(train_dataset)\n",
    "print('\\n')\n",
    "\n",
    "print(test_dataset)\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image dimension:(1, 28, 28)\n",
      "Image values between 0.0 and 1.0\n",
      "Label: 5\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get the first item in the training set and show properties\n",
    "\n",
    "item = train_dataset.__getitem__(0) # this is a tuple (tensor, label)\n",
    "img = item[0].numpy() \n",
    "print('Image dimension:%s'% (img.shape,)) # the first dimension is the channel, a color image could have 3 channels (R,G,B)\n",
    "print('Image values between %.1f and %.1f' % (np.amin(img), np.amax(img)))\n",
    "label = item[1]\n",
    "print('Label: %d' % label)\n",
    "plt.imshow(img[0,:])\n",
    "plt.gray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is everything you need to know for this lab session. If you want to learn more about data loading in PyTorch, you can have a look to [this tutorial](https://pytorch.org/tutorials/beginner/data_loading_tutorial.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational autoencoder\n",
    "\n",
    "Let $\\mathbf{x} \\in \\mathbb{R}^D$ and $\\mathbf{z} \\in \\mathbb{R}^K$ be two random vectors (with $K \\ll D$). \n",
    "\n",
    "**Generative model**: We consider the following generative model:\n",
    "\n",
    "\n",
    "$$p(\\mathbf{z}) = \\mathcal{N}(\\mathbf{0}, \\mathbf{I})$$\n",
    "\n",
    "$$p(\\mathbf{x} | \\mathbf{z} ; \\theta ) = \\mathcal{N}\\left( \\boldsymbol{\\mu}_\\theta(\\mathbf{z}), \\mathbf{I} \\right)$$\n",
    "\n",
    "The decoder outputs $\\boldsymbol{\\mu}_\\theta(\\mathbf{z})$. In theory, the mean of a Gaussian distribution lies in $]- \\infty, +\\infty[$. However, we know that our data (MNIST images) lie in $[0, 1]^D$, so we expect the mean of $p(\\mathbf{x} | \\mathbf{z} ; \\theta )$ to be in this interval too. This can be enforced by using a sigmoid activation function on the output layer.\n",
    "\n",
    "**Recognition model**: We consider the following recognition model:\n",
    "\n",
    "$$q(\\mathbf{z} | \\mathbf{x}; \\phi) = \\mathcal{N}\\left( \\boldsymbol{\\mu}_\\phi(\\mathbf{x}), \\text{diag}\\left\\{ \\mathbf{v}_\\phi(\\mathbf{x}) \\right\\} \\right)$$\n",
    "\n",
    "The encoder outputs $\\boldsymbol{\\mu}_\\phi(\\mathbf{x})$ and $\\mathbf{v}_\\phi(\\mathbf{x})$. A variance has to be positive, so a common practice consists in considering that the network ouputs the logarithm of the variance. You have to properly choose the activation function of the output layer of the encoder based on this information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, input_dim, encoding_dim):\n",
    "        super(VAE, self).__init__()\n",
    "        \n",
    "        self.encoder_fc1 = nn.Linear(input_dim, 512)\n",
    "        self.encoder_fc2 = nn.Linear(512, 128)\n",
    "        self.encoder_output_mean = nn.Linear(128, encoding_dim)\n",
    "        self.encoder_output_log_var = nn.Linear(128, encoding_dim)\n",
    "        \n",
    "        self.decoder_fc1 = nn.Linear(encoding_dim, 128)\n",
    "        self.decoder_fc2 = nn.Linear(128, 512)\n",
    "        self.decoder_output_mean = nn.Linear(512, input_dim)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def encode(self, x):\n",
    "        \"\"\"\n",
    "        From an input vector (MNIST image), this function computes \n",
    "        and returns the mean and log-variance of q(z|x).\n",
    "        \"\"\"\n",
    "\n",
    "        x = self.relu(self.encoder_fc1(x))\n",
    "        x = self.relu(self.encoder_fc2(x))\n",
    "        \n",
    "        return self.encoder_output_mean(x), self.encoder_output_log_var(x)\n",
    "        \n",
    "    def decode(self, z):\n",
    "        \"\"\"\n",
    "        From a latent vector, this function computes and returns \n",
    "        the mean of p(x|z).\n",
    "        \"\"\"\n",
    "\n",
    "        z = self.relu(self.decoder_fc1(z))\n",
    "        z = self.relu(self.decoder_fc2(z))\n",
    "        z = self.sigmoid(self.decoder_output_mean(z)) \n",
    "        \n",
    "        return z    \n",
    "    \n",
    "    def reparameterize(self, mu, log_var):\n",
    "        \"\"\"\n",
    "        From the mean and log-variance of q(z|x), this function returns a \n",
    "        latent vector drawn from q(z|x) using the reparametrization trick.\n",
    "        \"\"\"\n",
    "        \n",
    "        ######### TO COMPLETE #########\n",
    "\n",
    "        ###############################\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        mu, log_var = self.encode(x)\n",
    "        z = self.reparameterize(mu, log_var)\n",
    "        x_hat = self.decode(z)\n",
    "        \n",
    "        return x_hat, mu, log_var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Reparameterization\n",
    "\n",
    "In the forward, between encoding and decoding, we have to sample from the inference model using the reparametrization trick. \n",
    "\n",
    "You have to complete the ```reparameterize``` function (it's 3 lines of code). \n",
    "\n",
    "You can use the following PyTorch functions: [torch.randn_like](https://pytorch.org/docs/stable/torch.html#torch.randn_like) and [torch.exp](https://pytorch.org/docs/stable/torch.html#torch.exp)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now instantiate the model using a latent space of dimension 16."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAE(\n",
      "  (encoder_fc1): Linear(in_features=784, out_features=512, bias=True)\n",
      "  (encoder_fc2): Linear(in_features=512, out_features=128, bias=True)\n",
      "  (encoder_output_mean): Linear(in_features=128, out_features=16, bias=True)\n",
      "  (encoder_output_log_var): Linear(in_features=128, out_features=16, bias=True)\n",
      "  (decoder_fc1): Linear(in_features=16, out_features=128, bias=True)\n",
      "  (decoder_fc2): Linear(in_features=128, out_features=512, bias=True)\n",
      "  (decoder_output_mean): Linear(in_features=512, out_features=784, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "encoding_dim = 16\n",
    "input_dim = 784 # images of 28 x 28 pixels\n",
    "\n",
    "model = VAE(input_dim, encoding_dim)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Loss function\n",
    "\n",
    "Complete the following cell to define the loss function. It corresponds to the negative ELBO (refer to the slides), and remember that we set the variance of the generative model to 1.\n",
    "\n",
    "You have to use:\n",
    "\n",
    "- [```torch.sum()```](https://pytorch.org/docs/stable/torch.html#torch.sum) which returns the sum of all elements in the input tensor.\n",
    "- [```torch.exp()```](https://pytorch.org/docs/stable/torch.html#torch.exp) which returns a new tensor with the exponential of the elements of the input tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def VAE_loss(x, x_hat, mu, log_var):\n",
    "    \n",
    "    batch_size = x.shape[0]\n",
    "    \n",
    "    ############### TO COMPLETE ##############\n",
    "\n",
    "    ##########################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the following cell which implements the training loop to learn the model parameters. You should observe a nice decrease of the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 10 # number of epochs\n",
    "learning_rate = 0.001 # learning rate\n",
    "train_loss = [] # to store the loss after each epoch\n",
    "\n",
    "# we use Adam, a more sophisticated version of gradient descent\n",
    "# for an overview of gradient descent algorithms: https://ruder.io/optimizing-gradient-descent/\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "model.train()\n",
    "\n",
    "# Main training loop\n",
    "for epoch in np.arange(n_epochs):\n",
    "\n",
    "    train_acc_loss = 0 # the loss accumulated (summed) over the mini-batches\n",
    "    \n",
    "    print('Epoch: {}'.format(epoch+1))    \n",
    "    \n",
    "    for batch_idx, (x, y) in enumerate(train_loader):\n",
    "\n",
    "        x = x.view([-1, 784]).to(device) # reshape the 28x28 images\n",
    "        batch_size = x.shape[0]\n",
    "        \n",
    "        optimizer.zero_grad() # reset the gradients\n",
    "\n",
    "        # do forward pass\n",
    "        x_hat, mu, log_var = model(x)\n",
    "\n",
    "        # compute loss\n",
    "        loss = VAE_loss(x, x_hat, mu, log_var)\n",
    "        \n",
    "        # do backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # do gradient descent step\n",
    "        optimizer.step()\n",
    "\n",
    "        train_acc_loss += loss.item() # accumulate the loss over the training examples\n",
    "    \n",
    "    # store the averaged training loss after each epoch\n",
    "    train_loss.append(train_acc_loss) \n",
    "    print('...loss: {:.6f}'.format(train_acc_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_loss)\n",
    "plt.title('training loss')\n",
    "plt.xlabel('epochs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reconstruction\n",
    "\n",
    "In the following cell, we take some images from the test dataset and pass them through the VAE in order to observe the quality of reconstructed images, after encoding-decoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_reconstructions_VAE(model, test_loader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generation\n",
    "\n",
    "Let's now generate new images!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_VAE(model, encoding_dim):\n",
    "    with torch.no_grad():\n",
    "        # sample a Gaussian random vector with zero mean and identity covariance matrix\n",
    "        z = torch.randn(1,encoding_dim).to(device)\n",
    "        # pass it through the decoder\n",
    "        x = model.decode(z).reshape(28,28).cpu()\n",
    "    return x\n",
    "    \n",
    "    \n",
    "n_grid = 10\n",
    "X_gen = np.zeros((n_grid, n_grid, 28, 28))\n",
    "\n",
    "for i in range(n_grid):\n",
    "        for j in range(n_grid):\n",
    "            X_gen[i,j,:,:] = sample_VAE(model, encoding_dim)\n",
    "    \n",
    "display_digits(X_gen, n_i=10, n_j=10, figsize=(10, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "You may have observed that the reconstruction quality is not that good (for those of you familar with standard autoencoders, it may seem that the quality is worse). This comes from the fact that the regularization term in the loss function constrains the latent vectors provided by the encoder to be approximately distributed according to a Gaussian distribution with zero mean and identity covariance matrix. Indeed, this term constrains $q(\\mathbf{z} | \\mathbf{x}; \\phi)$ to be not too far from $p(\\mathbf{z}) = \\mathcal{N}(\\mathbf{0}, \\mathbf{I})$, according to the KL divergence. This constraint somehow \"removes some freedom\" from the encoder, which may eventually affect the reconstruction quality. \n",
    "\n",
    "If you force the regularization term to be zero (just multiply it by 0 when you compute the loss before the backward), you should recover the reconstruction quality of the autoencoder. However, if you do this you will end up with poor generation results, because at test time (for generating new digits), what you provide to the encoder are samples drawn from $p(\\mathbf{z}) = \\mathcal{N}(\\mathbf{0}, \\mathbf{I})$.\n",
    "\n",
    "To sum up, **there is a trade-off between reconstruction and generation quality**, which comes from the two terms in the loss function. You may try to adjust this trade-off by introducing an hyper-parameter $\\beta \\ge 0$ multiplying the regularization term in the loss function.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Walking into the latent space\n",
    "\n",
    "Let us consider a univariate (i.e. 1D) Gaussian distribution $\\mathcal{N}(0, 1)$. We want to build a grid of \"equally-spaced\" points in this 1-dimensional Gaussian space. For that purpose, we first build a grid of linearly spaced points between 0 and 1, that we then map through the inverse [cumulative distribution function](https://en.wikipedia.org/wiki/Cumulative_distribution_function) (CDF) of the Gaussian. This procedure is illustrated in the following cell. We end up with samples on the x-axis that are distributed according to the target Gaussian distribution. In particular, the density of points increases as we get closer to the mean.\n",
    "\n",
    "This procedure is related to [inverse transform sampling](https://en.wikipedia.org/wiki/Inverse_transform_sampling)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from utils import illust_walk_latent_space\n",
    "\n",
    "illust_walk_latent_space()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now consider a 2-dimensional latent space, i.e. the latent variable $\\mathbf{z} = [z_1, z_2]^\\top$ is of dimension $K=2$.\n",
    "\n",
    "Remember that the prior is $p(\\mathbf{z}) = \\mathcal{N}(\\mathbf{0},\\mathbf{I})$, which is equivalent to saying that $z_1$ and $z_2$ are independent and identically distributed (i.i.d) according to $\\mathcal{N}(0, 1)$.\n",
    "\n",
    "Therefore, we can build a grid of \"equally-spaced\" points in this 2-dimensional Gaussian space by applying the same procedure as before, independently for $z_1$ and $z_2$.\n",
    "\n",
    "For each pair $(z_1, z_2)$ in this grid, we will map the corresponding vector $\\mathbf{z}$ through the trained decoder of the VAE, and show the reconstructed mean image. This will allow us to visualize the learned latent space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3\n",
    "\n",
    "We retrain the model with a latent space of dimension 2 and run the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if encoding_dim == 2:\n",
    "    \n",
    "    from scipy.stats import norm\n",
    "    \n",
    "    n_grid = 20\n",
    "\n",
    "    uniform_samples = np.linspace(0.01,0.99,n_grid) # linearly spaced points between 0 and 1\n",
    "    normal_samples = norm.ppf(uniform_samples) # map through inverse CDF\n",
    "    Z = np.zeros((n_grid, n_grid, encoding_dim))\n",
    "    X = np.zeros((n_grid, n_grid, 28, 28))\n",
    "\n",
    "    for i in range(n_grid):\n",
    "        for j in range(n_grid):\n",
    "            Z[i,j,0] = normal_samples[i]\n",
    "            Z[i,j,1] = normal_samples[j]\n",
    "            with torch.no_grad():\n",
    "                z_ij_tensor = torch.from_numpy(Z[i,j,:]).reshape(1,encoding_dim).type(torch.FloatTensor).to(device)\n",
    "                x_gen = model.decode(z_ij_tensor).reshape(28,28)\n",
    "                X[i,j,:,:] = x_gen.cpu().numpy()\n",
    "\n",
    "    display_digits(X, figsize=(10, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conditional generation (bonus exercise)\n",
    "\n",
    "So far, we had no control on the class of the generated digit. We do not know if we will generate a 1, or 0, or a 9, etc.\n",
    "\n",
    "A simple approach to be able to control this aspect of the generation consits in modifying the model as follows:\n",
    "\n",
    "Let $\\mathbf{x} \\in \\mathbb{R}^D$ and $\\mathbf{z} \\in \\mathbb{R}^K$ be two random vectors. Let $\\mathbf{y} \\in \\{0,1\\}^C$ be a one-hot encoding vector representing the class label (in our case, $C=10$).\n",
    "\n",
    "We now consider the following generative model conditioned on the class label:\n",
    "\n",
    "$$p(\\mathbf{z}) = \\mathcal{N}(\\mathbf{0}, \\mathbf{I})$$\n",
    "\n",
    "$$p(\\mathbf{x} | \\mathbf{z}, \\mathbf{y} ; \\theta ) = \\mathcal{N}\\left( \\boldsymbol{\\mu}_\\theta(\\mathbf{z}, \\mathbf{y}), \\mathbf{I} \\right)$$\n",
    "\n",
    "\n",
    "and the following recognition model:\n",
    "\n",
    "$$q(\\mathbf{z} | \\mathbf{x}, \\mathbf{y}; \\phi) = \\mathcal{N}\\left( \\boldsymbol{\\mu}_\\phi(\\mathbf{x}, \\mathbf{y}),  \\text{diag}\\left\\{ \\mathbf{v}_\\phi(\\mathbf{x}, \\mathbf{y}) \\right\\}  \\right)$$\n",
    "\n",
    "You can define a new class ``CVAE`` where the encoder and decoder will now also take as input this one-hot encoding vector. Just use the following function to concatenate two tensors:\n",
    "\n",
    "``torch.cat([x, y], dim=1))`` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CVAE(nn.Module):\n",
    "    def __init__(self, input_dim, encoding_dim, cat_dim):\n",
    "        \n",
    "        super(CVAE, self).__init__()\n",
    "        \n",
    "        ################## TO DO ##################\n",
    "\n",
    "        ##########################################\n",
    "        \n",
    "    def encode(self, x, y):\n",
    "        \n",
    "        ################## TO DO ##################\n",
    "\n",
    "        ##########################################\n",
    "    \n",
    "    def reparameterize(self, mu, log_var):\n",
    "        \n",
    "        ################## TO DO ##################\n",
    "\n",
    "        ##########################################\n",
    "    \n",
    "    def decode(self, z, y):\n",
    "        \n",
    "        ################## TO DO ##################\n",
    "\n",
    "        ##########################################\n",
    "        \n",
    "    def forward(self, x, y):\n",
    "        \n",
    "        mu, log_var = self.encode(x, y)\n",
    "        z = self.reparameterize(mu, log_var)\n",
    "        x_hat = self.decode(z, y)\n",
    "        \n",
    "        return x_hat, mu, log_var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We instantiate the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding_dim = 16\n",
    "input_dim = 784 # images of 28 x 28 pixels\n",
    "cat_dim = 10 # one hot vectors for 10 classes\n",
    "\n",
    "model = CVAE(input_dim, encoding_dim, cat_dim)\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the following cell to train the model. It is very similar to the above VAE training, except that we now use both the images and labels provided by ```train_loader```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 10 # number of digit classes\n",
    "n_epochs = 5 # number of epochs\n",
    "learning_rate = 0.001 # learning rate\n",
    "train_loss = [] # to store the loss after each epoch\n",
    "\n",
    "# we use Adam, a more sophisticated version of gradient descent\n",
    "# for an overview of gradient descent algorithms: https://ruder.io/optimizing-gradient-descent/\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "model.train()\n",
    "\n",
    "# Main training loop\n",
    "for epoch in np.arange(n_epochs):\n",
    "    \n",
    "    train_acc_loss = 0 # the loss accumulated (summed) over the mini-batches\n",
    "    \n",
    "    print('Epoch: {}'.format(epoch+1))    \n",
    "\n",
    "    for batch_idx, (x, y) in enumerate(train_loader):\n",
    "\n",
    "        x = x.view([-1, 784]).to(device) # reshape the 28x28 images\n",
    "        y = nn.functional.one_hot(y, num_classes=num_classes).type(torch.FloatTensor).to(device)\n",
    "        \n",
    "        ################## TO DO ##################\n",
    "        \n",
    "        ##########################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now sample new data, conditionnally on the class of digit!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_grid = 10\n",
    "X_gen = np.zeros((n_grid, n_grid, 28, 28))\n",
    "\n",
    "def sample_CVAE(model, encoding_dim, class_ind=0):\n",
    "    with torch.no_grad():\n",
    "        z = torch.randn(1,encoding_dim).type(torch.FloatTensor).to(device)\n",
    "        y = torch.tensor([class_ind]).type(torch.int64).to(device)\n",
    "        y_one_hot = nn.functional.one_hot(y, num_classes=10).type(torch.FloatTensor).to(device)\n",
    "        x = model.decode(z, y_one_hot).reshape(28,28).cpu()\n",
    "    return x\n",
    "    \n",
    "\n",
    "for i in range(n_grid):\n",
    "        for j in range(n_grid):\n",
    "            with torch.no_grad():\n",
    "                z = torch.randn(1,encoding_dim).type(torch.FloatTensor).to(device)\n",
    "                y = torch.tensor([i]).type(torch.int64).to(device)\n",
    "                y_one_hot = nn.functional.one_hot(y, num_classes=10).type(torch.FloatTensor).to(device)\n",
    "                X_gen[i,j,:,:] = sample_CVAE(model, encoding_dim,i)\n",
    "    \n",
    "display_digits(X_gen, n_i=10, n_j=10, figsize=(10, 10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
